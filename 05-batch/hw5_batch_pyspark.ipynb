{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9927ef-827f-404c-8c4f-6e272064716f",
   "metadata": {},
   "source": [
    "## Question 1.\n",
    "### Install Spark, Run PySpark, Create a local spark session, Execute spark.version.\n",
    "#### What's the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a8fc80-d470-40bb-8034-5235a795ffca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/06 21:42:57 WARN Utils: Your hostname, cutty-sandbox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/03/06 21:42:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/06 21:42:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/06 21:42:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Import pyspark library\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebc6941d-186a-493f-b8e0-6f8e1a7ea5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d44679-e0dd-414d-9c27-36e25aee0ed1",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "### Read the October 2024 Yellow into a Spark Dataframe. Repartition the Dataframe to 4 partitions and save it to parquet.\n",
    "#### What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "016ecfcc-7faa-48fe-94b1-c5c35f0af978",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-06 21:42:59--  https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet\n",
      "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 54.230.209.72, 54.230.209.126, 54.230.209.200, ...\n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|54.230.209.72|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 64346071 (61M) [binary/octet-stream]\n",
      "Saving to: ‘yellow_tripdata_2024-10.parquet’\n",
      "\n",
      "yellow_tripdata_202 100%[===================>]  61.36M  18.7MB/s    in 3.4s    \n",
      "\n",
      "2025-03-06 21:43:03 (17.9 MB/s) - ‘yellow_tripdata_2024-10.parquet’ saved [64346071/64346071]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the October 2024 Yellow dataset\n",
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e058418-0f82-47fb-8198-008eda4032e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the October 2024 Yellow into a Spark Dataframe\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .parquet('yellow_tripdata_2024-10.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "227b6ae3-5769-4636-82ea-aa5d5985dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition the Dataframe to 4 partitions and save it to parquet.\n",
    "df = df.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "856761f0-54b3-4f78-bb94-72eb2995b963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet('yellow/2024/10', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67c50152-c435-487d-9c79-8d60e91f4310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 90M\n",
      "-rw-r--r-- 1 cutty cutty 23M Mar  6 21:43 part-00000-66395b91-795d-46c0-9e44-d995113eb852-c000.snappy.parquet\n",
      "-rw-r--r-- 1 cutty cutty 23M Mar  6 21:43 part-00001-66395b91-795d-46c0-9e44-d995113eb852-c000.snappy.parquet\n",
      "-rw-r--r-- 1 cutty cutty 23M Mar  6 21:43 part-00002-66395b91-795d-46c0-9e44-d995113eb852-c000.snappy.parquet\n",
      "-rw-r--r-- 1 cutty cutty 23M Mar  6 21:43 part-00003-66395b91-795d-46c0-9e44-d995113eb852-c000.snappy.parquet\n",
      "-rw-r--r-- 1 cutty cutty   0 Mar  6 21:43 _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!ls -lh yellow/2024/10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a36a2ad-616e-44b2-ac01-a356ee754004",
   "metadata": {},
   "source": [
    "## Question 3.\n",
    "### Consider only trips that started on the 15th of October.\n",
    "#### How many taxi trips were there on the 15th of October?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d241d8fc-0aa1-4452-bf50-fe0c2661a51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('yellow/2024/10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1b025ad-aba7-4e1c-8b16-76474805183a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128893"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.filter(to_date(df.tpep_pickup_datetime) == '2024-10-15') \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac3d2d74-a360-41e6-8326-33524bead38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('yellow_tripdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a99468a0-49f3-4d81-bb53-3bc96ff1d779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  128893|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        count(*)\n",
    "    FROM\n",
    "        yellow_tripdata\n",
    "    WHERE\n",
    "        date(tpep_pickup_datetime) == '2024-10-15'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9360cbf9-d28b-4243-b4e1-9a1a5ad32de0",
   "metadata": {},
   "source": [
    "## Question 4.\n",
    "#### What is the length of the longest trip in the dataset in hours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fe591ed-4a4e-46bf-a23f-002e1b973a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|max(trip_in_hours)|\n",
      "+------------------+\n",
      "|162.61777777777777|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn('tpep_dropoff_datetime', to_timestamp(col('tpep_dropoff_datetime'))) \\\n",
    "    .withColumn('tpep_pickup_datetime', to_timestamp(col('tpep_pickup_datetime'))) \\\n",
    "    .withColumn('trip_in_hours', (col('tpep_dropoff_datetime').cast('long') - col('tpep_pickup_datetime').cast('long'))/3600)\n",
    "\n",
    "df2.select(max(df2.trip_in_hours)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bdcc6b3-9808-43e3-b193-18f2134f5fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|max(trip_in_hours)|\n",
      "+------------------+\n",
      "|162.61777777777777|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    WITH datetimestamping AS (\n",
    "        SELECT\n",
    "            CAST(tpep_dropoff_datetime AS TIMESTAMP) AS dropoff_datetimestamp,\n",
    "            CAST(tpep_pickup_datetime AS TIMESTAMP) AS pickup_datetimestamp\n",
    "        FROM \n",
    "            yellow_tripdata\n",
    "    ),\n",
    "    tripsinhours AS (\n",
    "        SELECT\n",
    "            ((CAST(dropoff_datetimestamp AS INT) - CAST(pickup_datetimestamp AS INT))/3600) AS trip_in_hours\n",
    "        FROM\n",
    "           datetimestamping \n",
    "    )\n",
    "    \n",
    "    SELECT\n",
    "        MAX(trip_in_hours)\n",
    "    FROM\n",
    "        tripsinhours        \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52f62b4-e07e-4749-8ad3-dc75f9d64da8",
   "metadata": {},
   "source": [
    "## Question 6.\n",
    "### Load the zone lookup data into a temp view in Spark: \n",
    "### wget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n",
    "#### Using the zone lookup data and the Yellow October 2024 data, what is the name of the LEAST frequent pickup location Zone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b39c2ea-54c0-44cf-9e40-0a155fbeac1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-06 21:43:28--  https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n",
      "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 54.230.209.200, 54.230.209.72, 54.230.209.126, ...\n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|54.230.209.200|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12331 (12K) [text/csv]\n",
      "Saving to: ‘taxi_zone_lookup.csv’\n",
      "\n",
      "taxi_zone_lookup.cs 100%[===================>]  12.04K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2025-03-06 21:43:29 (2.67 MB/s) - ‘taxi_zone_lookup.csv’ saved [12331/12331]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the zone lookup dataset\n",
    "!wget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33a0e882-7e50-40e0-b1ae-6727168e5f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the zone lookup into a Spark Dataframe\n",
    "df_zones = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc74e8a5-dc4a-4fa6-88f4-b91f78c59fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones.write.parquet('zones', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5618cd2-12d8-415e-a01d-45bf45e0a31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = spark.read.parquet('yellow/2024/10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fb0c890-20e0-49d3-86a6-444aa31eaf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read.parquet('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "667a8ead-0efe-4bd4-90f9-c09d99fde0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones.createOrReplaceTempView('taxi_zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52e1f4da-fba4-4008-b64b-29ae6131a15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         pickup_zone|\n",
      "+--------------------+\n",
      "|Governor's Island...|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    WITH yellow_taxi_zones AS (\n",
    "        SELECT \n",
    "            y.PULocationID,\n",
    "            t.Zone\n",
    "        FROM\n",
    "            yellow_tripdata y\n",
    "            JOIN taxi_zones t\n",
    "            ON y.PULocationID = t.LocationID\n",
    "        WHERE\n",
    "            service_zone LIKE '%Yell% Zone'\n",
    "    ),\n",
    "    zone_pickup_frequency AS (\n",
    "        SELECT\n",
    "            Zone as pickup_zone, COUNT(*) AS pickup_frequency\n",
    "        FROM\n",
    "            yellow_taxi_zones\n",
    "        GROUP BY\n",
    "            1\n",
    "    ),\n",
    "    least_frequent_pickup AS (\n",
    "        SELECT\n",
    "            MIN(pickup_frequency) least_pickup\n",
    "        FROM\n",
    "            zone_pickup_frequency\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "        z.pickup_zone\n",
    "    FROM\n",
    "        zone_pickup_frequency z,\n",
    "        least_frequent_pickup l\n",
    "    WHERE\n",
    "        z.pickup_frequency = l.least_pickup\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
